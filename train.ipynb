{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "EHO3zztM96fa"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, ConcatDataset\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "i-RP0Wo-97TR"
   },
   "outputs": [],
   "source": [
    "# Main Hyperparameters\n",
    "img_size = 32                           # Dimension of spatial axes of input images\n",
    "patch_size = 4                          # Patch size\n",
    "in_channels = 1                         # Dimension of input channels\n",
    "\n",
    "embed_dim = 256                         # Dimension of embeddings\n",
    "batch_size = 128                        # Number of batch\n",
    "epochs = 30                            # Number of epochs\n",
    "dim_c = 192                             # Dimension of 'code' vector\n",
    "dim_inter = 192                         # Dimension of intermediate feature vector\n",
    "\n",
    "ns = 5                                  # Number of 'scripts'\n",
    "ni = 8                                  # Number of 'function' iterations\n",
    "nl = 1                                  # Number of LOCs\n",
    "nf = 10                                  # Number of 'function's\n",
    "n_cls = 1                               # Number of CLS tokens\n",
    "n_heads = 6                             # Number of heads per LOC\n",
    "loc_features = 128                      # Number of features per LOC head\n",
    "\n",
    "type_inference_depth = 2                # Type Inference MLP depth\n",
    "type_inference_width = 192              # Type Inference MLP width \n",
    "treshold = 1.4                          # Trunctation Parameter\n",
    "signature_dim = 24                      # Dimension of type_space\n",
    "\n",
    "attn_prob = 0.0                         # Drop-out probability of ModAttn layer\n",
    "proj_drop = 0.0                         # Drop-out probability of Projection \n",
    "mlp_depth = 4             \n",
    "number_of_class_mnist = 10                         \n",
    "# Pretraining Hyperparameters # Dimension of input channels\n",
    "frozen_function_codes = False           # Required for pretraining\n",
    "frozen_function_signatures = False      # Required for pretraining\n",
    "\n",
    "# Optimization Hyperparameters          \n",
    "beta1 = 0.9                             # Adam Optimizer beta1 parameter\n",
    "beta2 = 0.999                           # Adam Optimizer beta2 parameter\n",
    "lr = 1e-3                               # Learning Rate\n",
    "warmup_steps = 20                       # Scheduler warm up steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X_hWA768-W_k"
   },
   "source": [
    "# Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.basic_layers import NeuralInterpreter, NeuralInterpreter_vision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P_BIZq2m98Hr"
   },
   "source": [
    "# Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "H3riC08z98XA"
   },
   "outputs": [],
   "source": [
    "from dataset import get_data_loader_mixed, get_data_loader_mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "K3WVsA_B-ZXU"
   },
   "outputs": [],
   "source": [
    "# Parameters for dataset\n",
    "datasetname = 'digits'\n",
    "root = '/depo/web490/2022/Cutify/assets/data/'\n",
    "batch_size = 64\n",
    "train_loader, valid_loader = get_data_loader_mnist(datasetname, root, batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hNPHUne2-lMw"
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "OqKWJdO3_k3l"
   },
   "outputs": [],
   "source": [
    "# Create Neural Interpreter for vision Task\n",
    "\n",
    "from models.basic_layers import NeuralInterpreter_vision\n",
    "\n",
    "\n",
    "model = NeuralInterpreter_vision(ns, ni, nf, embed_dim, dim_c, mlp_depth, n_heads,\n",
    "                type_inference_width, signature_dim, treshold,  # typematch params\n",
    "                dim_c, n_classes=10,\n",
    "                img_size=32, patch_size=4, in_channels=1, n_cls=1,\n",
    "                attn_prob=0, proj_prob=0, # dropout rate for attention block\n",
    "              ).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_n_params(model):\n",
    "    pp=0\n",
    "    for p in list(model.parameters()):\n",
    "        nn=1\n",
    "        for s in list(p.size()):\n",
    "            nn = nn*s\n",
    "        pp += nn\n",
    "    return pp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "423963"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(get_n_params(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bkTYOoKM_sku",
    "outputId": "bf5dae73-0136-496e-95b9-b4635dcbeb6f"
   },
   "outputs": [],
   "source": [
    "from train import *\n",
    "\n",
    "# Define Optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=(beta1, beta2))\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "scheduler = WarmupCosineSchedule(optimizer, warmup_steps=warmup_steps, t_total=epochs)\n",
    "\n",
    "# log directory => save checkpoints\n",
    "LOG_DIR = '/depo/web490/2022/Cutify/assets/data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize wandb\n",
    "\n",
    "wandb.init(project=\"Neural-Interpreter\", entity=\"metugan\")\n",
    "\n",
    "# Run train\n",
    "train(model, train_loader, valid_loader, criterion, optimizer, epochs, scheduler, LOG_DIR, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "Untitled11.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "1574755903cdb34dd59c92caa17a10f4d65534a19f0435b7dc632f13bd5d95f2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
