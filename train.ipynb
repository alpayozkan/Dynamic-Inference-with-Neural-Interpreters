{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "EHO3zztM96fa"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import DataLoader, ConcatDataset\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import make_grid\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "i-RP0Wo-97TR"
      },
      "outputs": [],
      "source": [
        "# Main Hyperparameters\n",
        "img_size = 32                           # Dimension of spatial axes of input images\n",
        "patch_size = 4                          # Patch size\n",
        "in_channels = 1                         # Dimension of input channels\n",
        "embed_dim = 256                         # Dimension of embeddings\n",
        "batch_size = 128                        # Number of batch\n",
        "epochs = 100                            # Number of epochs\n",
        "dim_c = 192                             # Dimension of 'code' vector\n",
        "dim_inter = 192                         # Dimension of intermediate feature vector\n",
        "ns = 1                                  # Number of 'scripts'\n",
        "ni = 8                                  # Number of 'function' iterations\n",
        "nl = 1                                  # Number of LOCs\n",
        "nf = 5                                  # Number of 'function's\n",
        "n_cls = 1                               # Number of CLS tokens\n",
        "n_heads = 4                             # Number of heads per LOC\n",
        "loc_features = 128                      # Number of features per LOC head\n",
        "type_inference_depth = 2                # Type Inference MLP depth\n",
        "type_inference_width = 192              # Type Inference MLP width \n",
        "treshold = 1.4                          # Trunctation Parameter\n",
        "signature_dim = 24                      # Dimension of type_space\n",
        "attn_prob = 0.0                         # Drop-out probability of ModAttn layer\n",
        "proj_drop = 0.0                         # Drop-out probability of Projection \n",
        "mlp_depth = 2             \n",
        "number_of_class_mnist = 10                         \n",
        "# Pretraining Hyperparameters # Dimension of input channels\n",
        "frozen_function_codes = False           # Required for pretraining\n",
        "frozen_function_signatures = False      # Required for pretraining\n",
        "\n",
        "# Optimization Hyperparameters          \n",
        "beta1 = 0.9                             # Adam Optimizer beta1 parameter\n",
        "beta2 = 0.999                           # Adam Optimizer beta2 parameter\n",
        "lr = 1e-8                               # Learning Rate\n",
        "warmup_steps = 20                       # Scheduler warm up steps\n",
        "\n",
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_hWA768-W_k"
      },
      "source": [
        "# Layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "KbspriNu97iv"
      },
      "outputs": [],
      "source": [
        "class PatchEmbedding(nn.Module):\n",
        "  '''\n",
        "  Given images are linearly embedded via Patch Embedding in order to get tokens.\n",
        "  Args:\n",
        "  -----  \n",
        "    img_size    [int]: Images are assumed to be square\n",
        "    patch_size  [int]: Images are divided into patches of size `patch size`\n",
        "    in_channels [int]: Number of input channels of given images\n",
        "    embed_dim   [int]: Final embedding dimension\n",
        "  Attributes:\n",
        "  -----------\n",
        "    n_patches   [int]:  Number of total patches at the end\n",
        "    projection  [Conv]: Patch extractor\n",
        "  '''\n",
        "\n",
        "  def __init__(self, img_size, patch_size, in_channels, embed_dim, n_cls):\n",
        "    super().__init__()\n",
        "    self.img_size = img_size\n",
        "    self.patch_size = patch_size\n",
        "    self.in_channels = in_channels\n",
        "    self.embed_dim = embed_dim\n",
        "    \n",
        "    self.n_patches = (img_size // patch_size) ** 2\n",
        "    self.projection = nn.Conv2d(in_channels = in_channels, \n",
        "                                out_channels = embed_dim, \n",
        "                                kernel_size = patch_size, \n",
        "                                stride = patch_size)\n",
        "    \n",
        "    self.cls_tokens = nn.Parameter(torch.zeros(1, n_cls, embed_dim))\n",
        "    self.pos_embed = nn.Parameter(torch.zeros(1, n_cls + self.n_patches, embed_dim))\n",
        "  \n",
        "  def forward(self, x):\n",
        "    '''\n",
        "    Args:\n",
        "    -----\n",
        "      x [Tensor(B x C x H x W)]: Input images\n",
        "    \n",
        "    Returns:\n",
        "    --------\n",
        "      projected [Tensor(B x N + CLS x E)] where N + CLS stands for n_patches + cls tokens & E stands for embed_dim\n",
        "    '''\n",
        "    batch_size = x.size(0)\n",
        "    x = self.projection(x).flatten(2).transpose(1, 2) \n",
        "    cls_tokens = self.cls_tokens.expand(batch_size, -1, -1)\n",
        "    x = torch.cat((cls_tokens, x), dim=1)\n",
        "    x = x + self.pos_embed\n",
        "    return x\n",
        "\n",
        "\n",
        "  \n",
        "class MLP(nn.Module):\n",
        "  '''\n",
        "  Type Inference MLP module. \n",
        "  \n",
        "  Args:\n",
        "  ----\n",
        "    in_features     [int]: Dimension of input features and output features\n",
        "    hidden_features [int]: Dimension of intermediate features\n",
        "    out_features    [int]: Dimension of the signature\n",
        "    \n",
        "  Returns:\n",
        "  -------\n",
        "    t [Tensor()]: Type vector\n",
        "  '''\n",
        "  def __init__(self, in_features, hidden_features, out_features):\n",
        "    super().__init__()\n",
        "    self.net = nn.Sequential(\n",
        "              nn.Linear(in_features, hidden_features),\n",
        "              nn.GELU(),\n",
        "              nn.Linear(hidden_features, out_features)\n",
        "              )\n",
        "\n",
        "  def forward(self, embeddings):\n",
        "    '''\n",
        "    Args:\n",
        "    ----\n",
        "      embeddings [Tensor(B x N x E)]: \n",
        "    Returns:\n",
        "    --------\n",
        "      type_vector [Tensor(B x N x S)] where S stands for signature dimension\n",
        "    '''\n",
        "    type_vector = self.net(embeddings)\n",
        "    return type_vector \n",
        "\n",
        "  \n",
        "  \n",
        "class TypeMatching(nn.Module):\n",
        "  '''\n",
        "  Enables the learned routing of input set through functions.\n",
        "    \n",
        "    1. Given a set of element x_i, extract its type vector t_i\n",
        "    2. Compute `Compatibility`\n",
        "    3. If this compatibility is larger than treshold, permit f_u to access x_i.\n",
        "  '''\n",
        "  def __init__(self, type_inference, funcSign, threshold):\n",
        "    super().__init__()\n",
        "    self.s = funcSign\n",
        "    self.threshold = threshold\n",
        "    self.type_inference = type_inference\n",
        "    self.register_parameter('sigma', nn.Parameter(torch.ones(1)))\n",
        "\n",
        "  def forward(self, x):\n",
        "    '''\n",
        "    Args:\n",
        "    -----\n",
        "      x [Tensor(B x N x E)]: Embeddings\n",
        "      s [Tensor(F x S)]\n",
        "    \n",
        "    Attributes:\n",
        "    -----------\n",
        "      t [Tensor(B x N x S)]\n",
        "      compatilibity_score [Tensor(B x F x N)]: Parallelized computation score of compatibility score. F stands for # Functions.\n",
        "      compatilibity_hat   [Tensor(B x F x N)]: Negative exponentiated version of compatibility score\n",
        "    '''\n",
        "    t = self.type_inference(x)\n",
        "    compatibility_hat = self.get_compatilibity_score(t, self.s)\n",
        "    \n",
        "    # Softmax \n",
        "    compatibility_norm = compatibility_hat.sum(dim=1).unsqueeze(1) + 1e-5\n",
        "    compatibility = torch.div(compatibility_hat, compatibility_norm)\n",
        "    \n",
        "    return compatibility\n",
        "\n",
        "  def get_compatilibity_score(self, t, s):\n",
        "    distance = (1 - t @ s.transpose(0, 1))\n",
        "    return torch.where(distance > self.threshold, torch.exp(-distance/self.sigma), torch.tensor(0, dtype=torch.float)).transpose(1, 2)\n",
        "\n",
        "\n",
        "class ModLin2D(nn.Module):\n",
        "    '''\n",
        "    2D implementation of ModLin: instead of `code` vector, operated on `code` matrix. Used in ModAttention.\n",
        "    Args:\n",
        "    ----\n",
        "      code  [Tensor(dcond x nf)]: Code matrix of a all `function`s.\n",
        "      dout  [int]: Dimension of the output of the projection.\n",
        "      din   [int]: Dimension of the input  of the projection.\n",
        "      dcond [int]: Dimension of the code vector.\n",
        "    \n",
        "    Attributes:\n",
        "    -----------\n",
        "      W_c [Tensor(din x dcond)]: Projection matrix of condition vector\n",
        "      b   [Tensor(dout)]:        bias vector \n",
        "      W   [Tensor(dout x din)]:  Projection matrix of conditioned vector\n",
        "    '''\n",
        "    def __init__(self, code, dout, din, dcond, w_c, W, b):\n",
        "      super().__init__()\n",
        "      self.c = code\n",
        "      \n",
        "      self.w_c = w_c\n",
        "      # self.register_parameter('w_c', nn.Parameter(torch.empty(din, dcond)))\n",
        "      \n",
        "      # interpreter\n",
        "      self.b = b\n",
        "      self.W = W\n",
        "      # self.register_parameter('b', nn.Parameter(torch.empty(dout)))\n",
        "      # self.register_parameter('W', nn.Parameter(torch.empty(dout, din)))\n",
        "      \n",
        "      self.norm = nn.LayerNorm(din)\n",
        "\n",
        "    def forward(self, x):\n",
        "      '''\n",
        "      Performs linear projection of embeddings in `din` dimensional space onto\n",
        "      `dout` dimensional space by fusing [conditioning] embeddings [x] with normalized `code`\n",
        "      vectors.\n",
        "      '''  \n",
        "      out = self.norm(torch.matmul(self.w_c, self.c).T).unsqueeze(1)\n",
        "      out = x * out\n",
        "      out = torch.matmul(out, self.W.transpose(0, 1))+self.b\n",
        "      return out\n",
        "   \n",
        "  \n",
        "class ModMLP(nn.Module):\n",
        "  '''\n",
        "  Combination of ModLin Layers with the GELU activation function.\n",
        "  Args:\n",
        "  ----\n",
        "    mlp_depth [int]: Number of ModLin layers\n",
        "    code     [Tensor(dcond x 1)]: Code vector of a `function`.\n",
        "    dout     [int]: Dimension of the output projection\n",
        "    din      [int]: Dimension of the input  projection\n",
        "    dcond    [int]: Dimension of the code vector\n",
        "    activ    [nn.Module]: Activation function applied after every ModLin Layer\n",
        "  \n",
        "  Attributes:\n",
        "  -----------\n",
        "    modlin_blocks [List[ModLin]]: Stack of ModLin layers \n",
        "  '''\n",
        "  def __init__(self, mlp_depth, code, dout, din, dcond, w_c, W, b, activ=nn.GELU):\n",
        "      super().__init__()\n",
        "      self.modlin_blocks = [ModLin2D(code, dout, din, dcond, w_c, W, b), activ()]\n",
        "      \n",
        "      for i in range(mlp_depth-1):\n",
        "        self.modlin_blocks.append(ModLin2D(code, dout, dout, dcond, w_c, W, b))\n",
        "        self.modlin_blocks.append(activ())\n",
        "     \n",
        "      self.modlin_blocks = nn.Sequential(*self.modlin_blocks)\n",
        "  \n",
        "  def forward(self, x):\n",
        "      out = self.modlin_blocks(x)\n",
        "      return out\n",
        "\n",
        "  \n",
        "class ModAttn(nn.Module):\n",
        "  '''\n",
        "  Parallelized Self-Attention Layer. \n",
        "  \n",
        "  Args:\n",
        "  ----\n",
        "    code_matrix [Tensor(dcond x nf)]: Code matrix of a all `function`s.\n",
        "    din         [int]: Dimension of the input  projection\n",
        "    dcond       [int]: Dimension of the code vector\n",
        "    n_heads     [int]: Number of attention heads\n",
        "    attn_prob   [float]: Drop-out rate\n",
        "    proj_prob   [float]: Drop-out rate\n",
        "  \n",
        "  Arguments:\n",
        "  ----------\n",
        "    qkv:      ModLin Layer to obtain Q, K, V matrices\n",
        "    head_dim: Dimension of each attention head\n",
        "    scale:    Scale factor of qk_T\n",
        "  \n",
        "  Returns:\n",
        "  --------\n",
        "    y: Tensor of size [B x nf x n_token x din]\n",
        "  '''\n",
        "  def __init__(self,  code_matrix, din, dcond, n_heads, w_c, W, b, W_qkv, b_qkv, \n",
        "                      attn_prob = 0.0, proj_prob = 0.0):\n",
        "    super().__init__()\n",
        "    self.C = code_matrix\n",
        "    self.qkv = ModLin2D(code_matrix, 3 * din, din, dcond, w_c, W_qkv, b_qkv)\n",
        "    self.n_heads = n_heads\n",
        "    self.head_dim = din // n_heads\n",
        "    self.scale = self.head_dim ** -0.5\n",
        "    self.proj = ModLin2D(code_matrix, din, din, dcond, w_c, W, b)\n",
        "    self.attn_drop = nn.Dropout(attn_prob)\n",
        "    self.proj_drop = nn.Dropout(proj_prob)\n",
        "    # code, dout, din, dcond\n",
        "\n",
        "  def forward(self, x, compatibility):\n",
        "    B, N, E = x.shape\n",
        "    # [768, 128, 5, 64]\n",
        "    qkv = self.qkv(x.unsqueeze(1)).permute(3, 0, 1, 2)\n",
        "    qkv = qkv.view(3, self.n_heads, self.head_dim, B, -1, N) # 3 x 4 x 256 x 128 x 5 x 64\n",
        "    qkv = qkv.permute(0, 3, 1, 4, 5, 2)\n",
        "    # B x Heads x nf x tokens x token_dim\n",
        "    q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "    qk_t = (q @ k.transpose(-2, -1)) * self.scale\n",
        "    attn = qk_t.softmax(dim=-1)\n",
        "    attn = self.attn_drop(attn)\n",
        "\n",
        "    # Create compatibility matrix\n",
        "    compat_matrix = compatibility.transpose(1, 2) @ compatibility\n",
        "    W_hat = attn * compat_matrix.unsqueeze(1).unsqueeze(1)\n",
        "    W = W_hat.softmax(dim = -1)\n",
        "    y_hat = (W @ v).permute(0, 2, 3, 1, 4) # [B x nf x n_tokens x n_heads x head_dim]\n",
        "    y_hat = y_hat.flatten(3)  \n",
        "    # Mix \n",
        "    y = self.proj(y_hat).squeeze(1)\n",
        "    y = self.proj_drop(y)\n",
        "    return y\n",
        "\n",
        "class LOC(nn.Module):\n",
        "  '''\n",
        "  Line of Code Layer\n",
        "  Composed of 1 attention + 1 MLP layers\n",
        "  '''\n",
        "  def __init__( self, code_matrix, din, dcond, n_heads, mlp_depth, typematch, w_c, W, b, W_qkv, b_qkv,\n",
        "                attn_prob=0, proj_prob=0) -> None:\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    self.typematch = typematch\n",
        "    self.norm1 = torch.nn.LayerNorm(din)\n",
        "    self.norm2 = torch.nn.LayerNorm(din)\n",
        "\n",
        "    self.modattn = ModAttn( code_matrix, din, dcond, n_heads, \n",
        "                            w_c, W, b, W_qkv, b_qkv,\n",
        "                            attn_prob, proj_prob)\n",
        "    self.modmlp = ModMLP( mlp_depth, code_matrix, din, din, dcond,\n",
        "                          w_c, W, b)\n",
        "\n",
        "  def forward(self, x):\n",
        "    compat_matrix = self.typematch(x)\n",
        "    # x = x.squeeze()\n",
        "    x_norm = self.norm1(x)\n",
        "    a_hat = self.modattn(x_norm, compat_matrix)\n",
        "    \n",
        "    compat_matrix = compat_matrix.unsqueeze(-1)\n",
        "    a = x.unsqueeze(1) + compat_matrix*a_hat\n",
        "    \n",
        "    b_hat = self.modmlp(self.norm2(a))\n",
        "    y = a + compat_matrix*b_hat\n",
        "    \n",
        "    # pool-LOC => eqn-11\n",
        "    y = x + torch.sum(compat_matrix*y, dim=1)\n",
        "    return y\n",
        "\n",
        "\n",
        "class Script(nn.Module):\n",
        "  '''\n",
        "  Script blocks composed of LOC blocks\n",
        "  \n",
        "  Assumption:\n",
        "  -----------\n",
        "    LOC is composed of 1 layer.\n",
        "  \n",
        "  Args:\n",
        "  -----\n",
        "    ni          [int]: Number of function iterations in a script\n",
        "    nf          [int]: Number of functions per iteration\n",
        "    code_matrix [Tensor(dcond x nf)]: Code matrix of a all `function`s.\n",
        "    din         [int]: Dimension of the input  projection\n",
        "    dcond       [int]: Dimension of the code vector\n",
        "    n_heads     [int]: Number of attention heads\n",
        "    mlp_depth   [int]: Number of MLP depths of LOC layer\n",
        "    typematch   [nn.Module]: TypeMatching Module\n",
        "    attn_prob   [float]: Drop-out rate\n",
        "    proj_prob   [float]: Drop-out rate\n",
        "  '''\n",
        "  \n",
        "  def __init__( self, ni, nf, din, dcond, n_heads, mlp_depth, \n",
        "                type_inference, threshold, code_dim, signature_dim,\n",
        "                W, b, W_qkv, b_qkv,\n",
        "                attn_prob=0, proj_prob=0) -> None:\n",
        "    super().__init__()\n",
        "    \n",
        "    # w_c shared among all functions in a script  \n",
        "    self.register_parameter('w_c', nn.Parameter(torch.empty(din, dcond)))\n",
        "    \n",
        "    self.register_parameter('funcsign_matrix', nn.Parameter(torch.empty(nf, signature_dim)))\n",
        "    self.register_parameter('code_matrix', nn.Parameter(torch.empty(code_dim, nf)))\n",
        "\n",
        "    self.typematch = TypeMatching(type_inference, self.funcsign_matrix, threshold)\n",
        "\n",
        "    self.locBlocks = []\n",
        "    for i in range(ni):\n",
        "      # add LOC layer\n",
        "      self.locBlocks.append(LOC(self.code_matrix, din, dcond, n_heads, mlp_depth, self.typematch, \n",
        "                                self.w_c, W, b, W_qkv, b_qkv, attn_prob, proj_prob))\n",
        "      \n",
        "    self.locBlocks = nn.Sequential(*self.locBlocks)\n",
        "  \n",
        "  def forward(self, x):\n",
        "    x = self.locBlocks(x)\n",
        "    return x\n",
        "    \n",
        "\n",
        "class NeuralInterpreter(nn.Module):\n",
        "  def __init__( self, ns, ni, nf, din, dcond, mlp_depth, nheads,\n",
        "                type_inference_width, signature_dim, threshold,  # typematch params\n",
        "                code_dim, \n",
        "                attn_prob=0, proj_prob=0, # dropout rate for attention block\n",
        "              ) -> None:\n",
        "    super().__init__()\n",
        "    # Function definition: f = (s,c)\n",
        "    # function signature matrix (can be kept fixed or can be learnt, but warning collapse problem\n",
        "\n",
        "    # interpreter that is shared among the whole architecture\n",
        "    # 2 separate interpreters: din->din, din->3.din (qkv attn)\n",
        "    self.register_parameter('W', nn.Parameter(torch.empty(din, din)))\n",
        "    self.register_parameter('b', nn.Parameter(torch.empty(din)))\n",
        "    self.register_parameter('W_qkv', nn.Parameter(torch.empty(3*din, din)))\n",
        "    self.register_parameter('b_qkv', nn.Parameter(torch.empty(3*din)))\n",
        "\n",
        "    # type inference \n",
        "    self.type_inference = MLP(din, type_inference_width, signature_dim)\n",
        "\n",
        "    self.scriptBlocks = []\n",
        "    for i in range(ns):\n",
        "      self.scriptBlocks.append(Script(ni, nf, din, dcond, nheads, \n",
        "                                      mlp_depth, self.type_inference, threshold, code_dim, signature_dim,\n",
        "                                      self.W, self.b, self.W_qkv, self.b_qkv,\n",
        "                                      attn_prob, proj_prob))\n",
        "    self.scriptBlocks = nn.Sequential(*self.scriptBlocks)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.scriptBlocks(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P_BIZq2m98Hr"
      },
      "source": [
        "# Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "H3riC08z98XA"
      },
      "outputs": [],
      "source": [
        "def get_data_loader(datasetname, root, batch_size):\n",
        "  '''\n",
        "  Digits dataset is a combination of three related datasets:\n",
        "      1. SVHN \n",
        "      2. MNISTM\n",
        "      3. MNIST\n",
        "  '''\n",
        "  if datasetname == 'digits':\n",
        "    \n",
        "    transform = transforms.Compose([\n",
        "              transforms.Resize((32, 32)),\n",
        "              transforms.ToTensor(),\n",
        "    ])\n",
        "\n",
        "    # Get the MNIST dataset\n",
        "    mnist_train = datasets.MNIST(root=root,\n",
        "                                train=True,\n",
        "                                transform= transform,\n",
        "                                download=True)\n",
        "    \n",
        "    mnist_test = datasets.MNIST(root=root,\n",
        "                                train=False,\n",
        "                                transform = transform,\n",
        "                                download=True)\n",
        "    \n",
        "    # Get the train loader\n",
        "    train_loader = DataLoader(\n",
        "        mnist_train,\n",
        "        batch_size = batch_size,\n",
        "        num_workers = 2,\n",
        "        pin_memory = True,\n",
        "        shuffle= True\n",
        "    )\n",
        "\n",
        "    # Get the test loader\n",
        "    test_loader = DataLoader(\n",
        "        mnist_test,\n",
        "        batch_size = batch_size,\n",
        "        num_workers = 2,\n",
        "        pin_memory = True,\n",
        "        shuffle= True\n",
        "    )\n",
        "  return train_loader, test_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "K3WVsA_B-ZXU"
      },
      "outputs": [],
      "source": [
        "# Parameters for dataset\n",
        "datasetname = 'digits'\n",
        "root = 'data/'\n",
        "batch_size = 64\n",
        "loader, test_loader = get_data_loader(datasetname, root, batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "transform = transforms.Compose([\n",
        "              transforms.Resize((32, 32)),\n",
        "              transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "              transforms.ToTensor()\n",
        "    ])\n",
        "    \n",
        "mnist_train = datasets.MNIST(root=root,\n",
        "                                train=True,\n",
        "                                transform= transform,\n",
        "                                download=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "60000"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(mnist_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "60032"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(loader)*batch_size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 694
        },
        "id": "9LlyGZuEGjPW",
        "outputId": "ce684147-ca37-4136-e098-b526e6ff6e92"
      },
      "outputs": [],
      "source": [
        "# for i in loader:\n",
        "#   print(type(i))\n",
        "#   break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[tensor([[[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          ...,\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
            "\n",
            "\n",
            "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          ...,\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
            "\n",
            "\n",
            "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          ...,\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
            "\n",
            "\n",
            "        ...,\n",
            "\n",
            "\n",
            "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          ...,\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
            "\n",
            "\n",
            "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          ...,\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.]]],\n",
            "\n",
            "\n",
            "        [[[0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          ...,\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.],\n",
            "          [0., 0., 0.,  ..., 0., 0., 0.]]]]), tensor([7, 1, 6, 5, 0, 9, 2, 2, 1, 8, 6, 7, 3, 0, 9, 4, 5, 4, 8, 5, 4, 9, 7, 6,\n",
            "        7, 0, 2, 8, 1, 5, 2, 2, 0, 4, 8, 3, 4, 8, 6, 1, 5, 0, 4, 6, 5, 2, 7, 8,\n",
            "        7, 8, 4, 0, 2, 1, 6, 2, 4, 6, 8, 7, 8, 2, 3, 1])]\n"
          ]
        }
      ],
      "source": [
        "for batch in loader:\n",
        "    print(((batch)))\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D7W2FwmQ_Wu0"
      },
      "source": [
        "# Scheduler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "WgvBWDcs_Yo5"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "\n",
        "class WarmupCosineSchedule(LambdaLR):\n",
        "    \"\"\" Linear warmup and then cosine decay.\n",
        "        Linearly increases learning rate from 0 to 1 over `warmup_steps` training steps.\n",
        "        Decreases learning rate from 1. to 0. over remaining `t_total - warmup_steps` steps following a cosine curve.\n",
        "        If `cycles` (default=0.5) is different from default, learning rate follows cosine function after warmup.\n",
        "    \"\"\"\n",
        "    def __init__(self, optimizer, warmup_steps, t_total, cycles=.5, last_epoch=-1):\n",
        "        self.warmup_steps = warmup_steps\n",
        "        self.t_total = t_total\n",
        "        self.cycles = cycles\n",
        "        super(WarmupCosineSchedule, self).__init__(optimizer, self.lr_lambda, last_epoch=last_epoch)\n",
        "\n",
        "    def lr_lambda(self, step):\n",
        "        if step < self.warmup_steps:\n",
        "            return float(step) / float(max(1.0, self.warmup_steps))\n",
        "        # progress after warmup\n",
        "        progress = float(step - self.warmup_steps) / float(max(1, self.t_total - self.warmup_steps))\n",
        "        return max(0.0, 0.5 * (1. + math.cos(math.pi * float(self.cycles) * 2.0 * progress)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNPHUne2-lMw"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "7OLMbgH1_TPU"
      },
      "outputs": [],
      "source": [
        "def train(model, patch_embed, loader, optimizer, scheduler, criterion):\n",
        "  # Get Scheduler\n",
        "  \n",
        "  # Zero out gradients\n",
        "  model.zero_grad()\n",
        "  \n",
        "  # Define tqdm\n",
        "  epoch_iterator = tqdm(loader,\n",
        "                        desc=\"Training (X / X Steps) (loss=X.X)\",\n",
        "                        bar_format=\"{l_bar}{r_bar}\",\n",
        "                        dynamic_ncols=True)\n",
        "  for i in tqdm(range(epochs)):\n",
        "    losses = []\n",
        "    for step, (img,labels) in enumerate(epoch_iterator):\n",
        "      img, labels = img.to(device), labels.to(device)\n",
        "\n",
        "      tokens = patch_embed(img)\n",
        "      tokens = tokens.to(device)\n",
        "      out = model(tokens)\n",
        "      loss = criterion(out, labels)\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      loss.backward()\n",
        "      losses.append(loss)\n",
        "\n",
        "      optimizer.step()\n",
        "      \n",
        "      scheduler.step()\n",
        "    \n",
        "    avg_loss = torch.mean(losses)\n",
        "    epoch_iterator.set_description(\n",
        "                          \"Training (%d / %d Steps) (loss=%2.5f)\" % (i, epochs, avg_loss.val))\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "OqKWJdO3_k3l"
      },
      "outputs": [],
      "source": [
        "from models.basic_layers import NeuralInterpreter_vision\n",
        "\n",
        "patch_embed = PatchEmbedding(img_size, patch_size, in_channels, embed_dim, n_cls).to(device)\n",
        "\n",
        "model = NeuralInterpreter_vision(ns, ni, nf, embed_dim, dim_c, mlp_depth, n_heads,\n",
        "                type_inference_width, signature_dim, treshold,  # typematch params\n",
        "                dim_c, n_classes=10,\n",
        "                attn_prob=0, proj_prob=0, # dropout rate for attention block\n",
        "              ).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bkTYOoKM_sku",
        "outputId": "bf5dae73-0136-496e-95b9-b4635dcbeb6f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Training (X / X Steps) (loss=X.X): 100%|| 938/938 [04:16<00:00,  3.66it/s]\n",
            "  0%|          | 0/100 [04:16<?, ?it/s]\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "mean(): argument 'input' (position 1) must be Tensor, not list",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[1;32m/home/finch/git/Dynamic-Inference-with-Neural-Interpreters/train.ipynb Cell 21'\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/finch/git/Dynamic-Inference-with-Neural-Interpreters/train.ipynb#ch0000013?line=3'>4</a>\u001b[0m scheduler \u001b[39m=\u001b[39m WarmupCosineSchedule(optimizer, warmup_steps\u001b[39m=\u001b[39mwarmup_steps, t_total\u001b[39m=\u001b[39mepochs)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/finch/git/Dynamic-Inference-with-Neural-Interpreters/train.ipynb#ch0000013?line=5'>6</a>\u001b[0m \u001b[39m# Run train\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/finch/git/Dynamic-Inference-with-Neural-Interpreters/train.ipynb#ch0000013?line=6'>7</a>\u001b[0m train(model, patch_embed, loader, optimizer, scheduler, criterion)\n",
            "\u001b[1;32m/home/finch/git/Dynamic-Inference-with-Neural-Interpreters/train.ipynb Cell 19'\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, patch_embed, loader, optimizer, scheduler, criterion)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/finch/git/Dynamic-Inference-with-Neural-Interpreters/train.ipynb#ch0000011?line=25'>26</a>\u001b[0m   optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/finch/git/Dynamic-Inference-with-Neural-Interpreters/train.ipynb#ch0000011?line=27'>28</a>\u001b[0m   scheduler\u001b[39m.\u001b[39mstep()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/finch/git/Dynamic-Inference-with-Neural-Interpreters/train.ipynb#ch0000011?line=29'>30</a>\u001b[0m avg_loss \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mmean(losses)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/finch/git/Dynamic-Inference-with-Neural-Interpreters/train.ipynb#ch0000011?line=30'>31</a>\u001b[0m epoch_iterator\u001b[39m.\u001b[39mset_description(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/finch/git/Dynamic-Inference-with-Neural-Interpreters/train.ipynb#ch0000011?line=31'>32</a>\u001b[0m                       \u001b[39m\"\u001b[39m\u001b[39mTraining (\u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m / \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m Steps) (loss=\u001b[39m\u001b[39m%2.5f\u001b[39;00m\u001b[39m)\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m%\u001b[39m (i, epochs, avg_loss\u001b[39m.\u001b[39mval))\n",
            "\u001b[0;31mTypeError\u001b[0m: mean(): argument 'input' (position 1) must be Tensor, not list"
          ]
        }
      ],
      "source": [
        "# Define Optimizer\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr, betas=(beta1, beta2))\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "scheduler = WarmupCosineSchedule(optimizer, warmup_steps=warmup_steps, t_total=epochs)\n",
        "\n",
        "# Run train\n",
        "train(model, patch_embed, loader, optimizer, scheduler, criterion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5o4OJlZv_vYh"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Untitled11.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.13 ('gan')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "1574755903cdb34dd59c92caa17a10f4d65534a19f0435b7dc632f13bd5d95f2"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
